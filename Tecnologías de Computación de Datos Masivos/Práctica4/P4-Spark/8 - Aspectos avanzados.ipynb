{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aspectos avanzados\n",
    "==================\n",
    "\n",
    "### Acumuladores\n",
    "\n",
    "Permiten agregar valores desde los *worker nodes*, que se pasan al\n",
    "*driver*\n",
    "\n",
    "-   Útiles para contar eventos\n",
    "\n",
    "-   Solo el driver puede acceder a su valor\n",
    "\n",
    "-   Acumuladores usados en transformaciones de RDDs pueden ser\n",
    "    incorrectos\n",
    "\n",
    "    -   Si el RDD se recalcula, el acumulador puede actualizarse\n",
    "\n",
    "    -   En acciones, este problema no ocurre\n",
    "\n",
    "-   Por defecto, los acumuladores son enteros o flotantes\n",
    "    - Es posible crear “acumuladores a medida” usando [`AccumulatorParam`](https://spark.apache.org/docs/1.5.2/api/python/pyspark.html#pyspark.AccumulatorParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from test_helper import Test\n",
    "from __future__ import print_function\n",
    "\n",
    "from numpy.random import randint\n",
    "npares = sc.accumulator(0)\n",
    "def esPar(n):\n",
    "    global npares\n",
    "    if n%2 == 0:\n",
    "        npares += 1\n",
    "rdd = sc.parallelize(randint(100, size=10000))\n",
    "rdd.foreach(esPar)\n",
    "print(\"N pares: %d\" % npares.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables de broadcast\n",
    "\n",
    "-   Por defecto, todas las variables compartidas son enviadas a todos\n",
    "    los workers\n",
    "\n",
    "    -   Se reenvían en cada operación en la que aparezcan\n",
    "\n",
    "-   Variables de broadcast: permiten enviar de forma eficiente variables\n",
    "    de solo lectura a los workers\n",
    "\n",
    "    -   Se envían solo una vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dicc=sc.broadcast({\"a\":\"alpha\",\"b\":\"beta\",\"c\":\"gamma\"})\n",
    "\n",
    "rdd=sc.parallelize([(\"a\", 1),(\"b\", 3),(\"a\", -4),(\"c\", 0)])\n",
    "reduced_rdd = rdd.reduceByKey(lambda x,y: x+y).map(lambda (x,y): (dicc.value[x],y))\n",
    "\n",
    "Test.assertEquals(reduced_rdd.collect(), [('alpha',-3), ('gamma',0), ('beta',3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajando a nivel de partición\n",
    "\n",
    "Una operación map se hace para cada elemento de un RDD\n",
    "\n",
    "-   Puede implicar operaciones redundantes (p.e. abrir una conexión a\n",
    "    una BD)\n",
    "\n",
    "-   Puede ser poco eficiente\n",
    "\n",
    "Se pueden hacer `map` y `foreach` una vez por partición:\n",
    "\n",
    "-   Métodos `mapPartitions()`, `mapPartitionsWithIndex()` y\n",
    "    `foreachPartition()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1,2,3,4,5,6,7,8,9], 2)\n",
    "print(nums.glom().collect())\n",
    "\n",
    "def sumayCuenta(nums):\n",
    "    sumaCuenta = [0,0]\n",
    "    for num in nums:\n",
    "        sumaCuenta[0] += num\n",
    "        sumaCuenta[1] += 1\n",
    "    return sumaCuenta\n",
    "    \n",
    "print(nums.mapPartitions(sumayCuenta).glom().collect())\n",
    "\n",
    "def sumayCuentaIndex(index, nums):\n",
    "    return index,sumayCuenta(nums)\n",
    "    \n",
    "print(nums.mapPartitionsWithIndex(sumayCuentaIndex).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexión con programas externos (*Piping*)\n",
    "\n",
    "Pipes con programas en otros lenguajes que puedan leer de la entrada\n",
    "estándar y escribir en la salida estándar\n",
    "\n",
    "-   Método `pipe()`: lee de stdin y escribe en stdout\n",
    "\n",
    "-   Cada elemento de un RDD se lee/escribe como String\n",
    "\n",
    "-   El script debe ser facilitado a todos los nodos de cómputo\n",
    "    $\\Rightarrow$ `SparkContext.addFile(path)`, `SparkFiles.get(path)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.files import SparkFiles\n",
    "rdd = sc.parallelize([\"uno\", \"dos\", \"tres\", \"cuatro\"])\n",
    "script = \"aux/echoupper.sh\"\n",
    "scriptName = \"echoupper.sh\"\n",
    "sc.addFile(script)\n",
    "pipeRDD = rdd.pipe(SparkFiles.get(scriptName))\n",
    "print(pipeRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
