{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecución de un script en PySpark\n",
    "==========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Ejecución de un programa Spark\n",
    "\n",
    "#### Shells Python y Scala\n",
    "\n",
    "-   Scala: `$ bin/spark-shell`\n",
    "\n",
    "-   Python: `$ bin/pyspark`\n",
    "\n",
    "-   IPython:\n",
    "    `$ export PYSPARK_DRIVER_PYTHON=ipython; bin/pyspark`\n",
    "\n",
    "-   IPython Notebook:\n",
    "    `$ export PYSPARK_DRIVER_PYTHON=ipython; export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"; bin/pyspark`\n",
    "\n",
    "#### Ejecución con `spark-submit`\n",
    "\n",
    "-   Permite lanzar programas Spark a un cluster\n",
    "\n",
    "-   Ejemplo:\n",
    "\n",
    "    `$ bin/spark-submit --master yarn --deploy-mode cluster \\  \n",
    "     --py-files otralib.zip,otrofich.py \\  \n",
    "     --num-executors 10 --executor-cores 2 \\  \n",
    "     mi-script.py opciones_del_script`\n",
    "\n",
    "#### Opciones de `spark-submit`\n",
    "\n",
    "-   `master`: cluster manager a usar (opciones: `yarn`,\n",
    "    `mesos://host:port`, `spark://host:port`, `local`)\n",
    "\n",
    "-   `deploy-mode`: dos modos de despliegue\n",
    "\n",
    "    -   `client`: ejecuta el driver en el nodo local\n",
    "\n",
    "    -   `cluster`: ejecuta el driver en un nodo del cluster\n",
    "\n",
    "-   `class`: clase a ejecutar (Java o Scala)\n",
    "\n",
    "-   `name`: nombre de la aplicación (se muestra en el Spark web)\n",
    "\n",
    "-   `jars`: ficheros jar a añadir al classpath (Java o Scala)\n",
    "\n",
    "-   `py-files`: archivos a añadir al PYTHONPATH (`.py`,`.zip`,`.egg`)\n",
    "\n",
    "-   `files`: ficheros de datos para la aplicación\n",
    "\n",
    "-   `executor-memory`: memoria total de cada ejecutor\n",
    "\n",
    "-   `driver-memory`: memoria del proceso driver\n",
    "\n",
    "Para más opciones: `spark-submit --help`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros de configuración\n",
    "\n",
    "Diversos parámetros ajustables en tiempo de ejecución\n",
    "\n",
    "-   En un script\n",
    "\n",
    "```python\n",
    "from pyspark import SparkConf,SparkContext\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", \"Mi apli\")\n",
    "conf.set(\"spark.master\", \"local[2]\") # 2 cores\n",
    "conf.set(\"spark.ui.port\", \"3600\")    # Defecto: 4040\n",
    "sc = SparkContext(conf=conf)\n",
    "```\n",
    "\n",
    "-   Mediante flags en el `spark-submit`\n",
    "\n",
    "    `$ bin/spark-submit --master local[2] --name \"Mi apli\" \\  \n",
    "    --conf spark.ui.port=3600 mi-script.py`\n",
    "    \n",
    "    \n",
    "-   Mediante un fichero de propiedades\n",
    "    \n",
    "    `$ cat config.conf \n",
    "    spark.master     local[2] \n",
    "    spark.app.name   \"Mi apli\" \n",
    "    spark.ui.port 3600`\n",
    "    \n",
    "    `$ bin/spark-submit --properties-file config.conf mi-script.py`\n",
    "\n",
    "Más info:\n",
    "<http://spark.apache.org/docs/latest/configuration.html#spark-properties>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajos, etapas y tareas\n",
    "-   Un programa Spark define un DAG de RDDs\n",
    "\n",
    "    -   Las transformaciones crean RDDs hijos a partir de RDDs padres\n",
    "\n",
    "-   Las acciones traducen el DAG en un plan de ejecución\n",
    "\n",
    "    -   El driver envía un *trabajo* (job) para computar todos los RDDs\n",
    "        implicados en la acción\n",
    "\n",
    "    -   El job se descompone en una o más *etapas* (stages)\n",
    "\n",
    "    -   Cada stage corresponde a uno o más RDDs del DAG (*pipelining*)\n",
    "\n",
    "    -   Las stages se procesan en orden, lanzándose *tareas* (tasks)\n",
    "        individuales que computan segmentos de los RDDs\n",
    "\n",
    "-   Pipelining: varios RDDs se computan en una misma stage\n",
    "\n",
    "    -   Si los RDDs se pueden obtener de sus padres sin movimiento de\n",
    "        datos (p.e. *map*)\n",
    "\n",
    "    -   Si un RDD se ha *cacheado* en memoria o disco\n",
    "\n",
    "    -   En el [interfaz web de Spark](http://localhost:4040 \"PySpark en localhost\") se muestran el número de stages por\n",
    "        job (más info: método `toDebugString()` de los RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "rdd = sc.parallelize(xrange(1000)).cache()\n",
    "rdd2 = (rdd.map(lambda x: (x, 2*x))\n",
    "           .map(lambda (x,y): (x-100, y**2))\n",
    "           .reduceByKey(lambda x,y: x+y)\n",
    "           .values())\n",
    "r = rdd2.reduce(lambda x,y:x+y)\n",
    "print(rdd2.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 5\n",
    "\n",
    "Objetivo:\n",
    "\n",
    "Desarrollar un script Pyspark, que haga dos operaciones, a partir de los ficheros cite75_99.txt y los ficheros secuence creados en la práctica anterior (a partir del fichero apat63_99.txt)\n",
    "\n",
    "1. Realizad un Join similar al de Hadoop, para obtener, para cada patente, el país y el número de citas.\n",
    "2. A partir de la información de las patentes (fichero sequence), obtener una salida de la forma:\n",
    "\n",
    "    <tt>país     año -> no de patentes ese año; año -> no de patentes ese año; ....</tt>\n",
    "    \n",
    "    por ejemplo:\n",
    "    \n",
    "    <tt>ES      1963->26;1964->19;1965->49;...</tt>\n",
    "    \n",
    "    La salida debe estar ordenada por países, y para cada país por año.\n",
    "\n",
    "Tened en cuenta lo siguiente:\n",
    "\n",
    "- Se debe de crear un único script que haga los dos apartados en orden (usando persistencia de datos).\n",
    "- En el Join realizad un join completo (full outer).\n",
    "- Los datos de salida deben almacenarse  en ficheros de texto, en dos directorios separados, uno por apartado.\n",
    "\n",
    "Plantilla a utilizar\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "from pyspark import SparkContext,StorageLevel\n",
    "#\n",
    "# Fichero spark que implementa el codigo SimpleReduceSideJoin y sortsecundario2\n",
    "# Entrada: fichero cite75_99.txt y el/los ficheros sequence creados en la práctica creasequencefile\n",
    "#\n",
    "# Ejecutar en local con:\n",
    "# spark-submit --master local[*] sparkpractica.py dir-cite75_99.txt dir-apat63_99.seq outdir1 outdir2\n",
    "#\n",
    "# Y en el cluster\n",
    "# spark-submit --master yarn sparkpractica.py dir-cite75_99.txt dir-apatseq outdir1 outdir2\n",
    "\n",
    "def main():\n",
    "    # Parámetros de entrada:\n",
    "    # argv[1] = PATH al fichero cite75_99.txt\n",
    "    # argv[2] = PATH a los ficheros sequence creados en la practica anterior\n",
    "    # argv[3] = directorio de salida de Join\n",
    "    # argv[4] = directorio de salida de Sort\n",
    "    if len(sys.argv) != 5:\n",
    "        print(\"Usage: sparkjoin.py file_citas.txt file(s)_apatseq outdir1 outdir2\", file=sys.stderr)\n",
    "        exit(-1)\n",
    "    \n",
    "    sc = SparkContext(appName=\"Spark practica\")\n",
    "    \n",
    "    # Lee como un RDD el fichero cite75_99.txt\n",
    "    citas = sc.textFile(sys.argv[1])\n",
    "\n",
    "    # Elimina la cabecera y separa cada línea por la coma\n",
    "    # TODO: Para cada patente citada genera un par clave/valor (patente,1) y acumula los 1s para cada clave\n",
    "    num_citas = \n",
    "    \n",
    "    # num_citas tiene el numero de veces que se ha citado una patente\n",
    "\n",
    "    # Cargo los ficheros sequence (clave=país valor=n_patente,año ambos de tipo Text)\n",
    "    info = \n",
    "    \n",
    "    # TODO: Cachearlo porque lo voy a usar dos veces\n",
    "    info.\n",
    "    \n",
    "    # TODO: Crea un RDD con clave el n_patente y valor el país\n",
    "    pat_info = \n",
    "    \n",
    "    # Hago un full outer join de num_citas y pat_info\n",
    "    pat_country = \n",
    "    \n",
    "    # Salvo en outdir1\n",
    "    pat_country.saveAsTextFile(sys.argv[3])\n",
    "\n",
    "    # La parte de sort (pais   año->n_patentes;año->n_patentes;...)\n",
    "    # Primero coge el fichero info y hace el map: (pais patente,año) -> (pais,año 1)\n",
    "    # luego reduce sumando los 1 y ordena por clave (para tener los años ordenados)\n",
    "    # el siguiente map hace: (pais,año n) -> (pais año->n)\n",
    "    # luego reduce por país para obtener (pais anho1->n1;anho2->n2;...)\n",
    "    # y ordena otra vez por países\n",
    "    country_year =     \n",
    "    # ¿Hace esto lo que queremos?\n",
    "\n",
    "    # salvo en outdir2\n",
    "    country_year.saveAsTextFile(sys.argv[4])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
