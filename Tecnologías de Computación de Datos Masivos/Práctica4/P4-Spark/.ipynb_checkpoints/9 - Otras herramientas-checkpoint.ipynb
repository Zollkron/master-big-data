{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras herramientas\n",
    "=========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "-   Interfaz para trabajar con datos estructurados y semiestructurados\n",
    "\n",
    "-   Capacidades principales\n",
    "\n",
    "    -   Lee datos de una gran variedad de fuentes: JSON, Hive, Parquet…\n",
    "\n",
    "    -   Permite consultas SQL, tanto desde programas Spark como externas\n",
    "        usando conectores estándar (JDBC/ODBC)\n",
    "\n",
    "    -   Integra SQL y código Spark normal (en Python/Java/Scala)\n",
    "\n",
    "### Conceptos básicos de Spark SQL\n",
    "\n",
    "-   Contexto: punto de entrada\n",
    "\n",
    "    -   `SQLContext`: Contexto básico\n",
    "\n",
    "    -   `HiveContext`: Contexto basado en Hive\n",
    "\n",
    "-   `DataFrame`: colección distribuida de datos organizada en columnas\n",
    "    con nombre\n",
    "\n",
    "    -   Colección distribuida de datos organizacos en columnas con nombre\n",
    "\n",
    "    -   Conceptualmente equivalente a una tabla en una BD o a un\n",
    "        dataframe en R o Pandas\n",
    "\n",
    "-   `DataSet`: nuevo tipo de datos añadido en Spark 1.6\n",
    "    - Intenta proporcionar los beneficioos de los RDDs con las optimizaciones que proporciona el motor de ejecución de Spark SQL\n",
    "    - De momento, sólo disponible en Scala y Java\n",
    "    - En [Java](http://spark.apache.org/docs/latest/api/java/index.html \"Interface Row\") y [Scala](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row \"trait Row extends Serializable\"), un DataFrame es un DataSet de Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL: ejemplo fichero JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from test_helper import Test\n",
    "\n",
    "from pyspark.sql import SQLContext, Row\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.json(\"datos/gente.json\")\n",
    "\n",
    "print(\"Tabla completa\")\n",
    "df.show()\n",
    "\n",
    "print(\"Estadísticas\")\n",
    "df.describe(\"edad\").show()\n",
    "\n",
    "print(\"Esquema inferido\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"Columna nombre\")\n",
    "df.select(\"nombre\").show()\n",
    "\n",
    "print(\"Filtra por edad\")\n",
    "df.filter(df[\"edad\"] > 21).show()\n",
    "\n",
    "print(\"Agrupa por edad\")\n",
    "df.groupBy(\"edad\").count().show()\n",
    "\n",
    "print(\"Consulta SQL\")\n",
    "# Registra la tabla para usar SQL\n",
    "df.registerTempTable(\"gente\")\n",
    "teens = sqlContext.sql(\"SELECT nombre FROM gente WHERE edad >= 13 AND edad <= 19\")\n",
    "teens.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"datos/gente.txt\")\n",
    "p = lines.map(lambda l: l.split(\",\"))\n",
    "ne = p.map(lambda p: Row(nombre=p[0], edad=int(p[1])))\n",
    "df = sqlContext.createDataFrame(ne)                              \n",
    "\n",
    "print(\"Tabla completa\")\n",
    "df.show()\n",
    "\n",
    "print(\"Consulta SQL\")\n",
    "# Registra la tabla para usar SQL\n",
    "df.registerTempTable(\"gente2\")\n",
    "teens2 = sqlContext.sql(\"SELECT nombre FROM gente2 WHERE edad >= 13 AND edad <= 19\")\n",
    "teens2.show()\n",
    "\n",
    "# Añade la tabla a la cache\n",
    "sqlContext.cacheTable(\"gente2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL: UDFs\n",
    "\n",
    "-   Funciones definidas por el usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Registra una función que devuelve \n",
    "# un entero con la longitud del string\n",
    "sqlContext.registerFunction(\"lon\", lambda x: len(x), IntegerType())\n",
    "\n",
    "# Aplica la función a la columna nombre                             \n",
    "lndf = sqlContext.sql(\"SELECT lon(nombre) FROM gente2\")\n",
    "print(\"Número de caracteres de los nombres\")\n",
    "lndf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming\n",
    "\n",
    "-   Procesamiento escalable, *high-throughput* y tolerante a fallos de\n",
    "    flujos de datos\n",
    "\n",
    "<img src=\"figs/streaming-flow.png\" alt=\"Flujo de Spark Streaming\" style=\"width: 800px;\"/>\n",
    "\n",
    "-   Entrada desde muchas fuentes: Kafka, Flume, Twitter, ZeroMQ, Kinesis\n",
    "    o sockets TCP\n",
    "\n",
    "### Arquitectura de Spark Streaming\n",
    "\n",
    "Abstracción principal: DStream (*discretized stream*).\n",
    "\n",
    "-   Representa un flujo continuo de datos\n",
    "\n",
    "![image](figs/dstreams.png)\n",
    "\n",
    "Arquitectura *micro-batch*\n",
    "\n",
    "-   Los datos recibidos se agrupan en batches\n",
    "\n",
    "-   Los batches se crean a intervalos regulares (batch interval)\n",
    "\n",
    "-   Cada batch forma un RDD, que es procesado por SPARK\n",
    "\n",
    "-   Adicionalmente: transformaciones con estado mediante\n",
    "\n",
    "    -   Operaciones con ventanas\n",
    "\n",
    "    -   Tracking del estado por cada clave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming: WordCount en red\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Contexto con dos threads locales\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
    "sc.setLogLevel(\"WARN\")\n",
    "# Contexto Streaming con un batch interval de 1 s\n",
    "ssc = StreamingContext(sc, 5)\n",
    "\n",
    "# DStream que conecta a localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Ejecuta un WordCount\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "              .map(lambda word: (word, 1))\\\n",
    "              .reduceByKey(lambda a, b: a+b)\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start() # Inicia la computacion\n",
    "ssc.awaitTermination() # Espera a que termine\n",
    "```\n",
    "Para ejecutar el ejemplo:\n",
    "\n",
    "   1. En un terminal usa netcat como un servidor en el puerto 9999\n",
    "\n",
    "    `$ nc -lk 9999`\n",
    "\n",
    "   2. En otro terminal, inicia el script Spark\n",
    "\n",
    "    `$ spark-submit netwc.py localhost 9999`\n",
    "\n",
    "   3. Escribe líneas en el terminal del netcat, que serán recogidas y procesadas por el script\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming: WordCount con estado\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "sc = SparkContext(\"local[2]\", \"StateWordCount\")\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"cpdir\") # Activa checkpoint\n",
    "\n",
    "def updateFunc(new_values, last_sum):\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "              .map(lambda word: (word, 1))\\\n",
    "              .updateStateByKey(updateFunc)\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start() # Inicia la computacion\n",
    "ssc.awaitTermination() # Espera a que termine\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## MLlib: Machine Learning Library\n",
    "\n",
    "Librería de algoritmos paralelos de ML para datos masivos\n",
    "\n",
    "-   Algoritmos de clasificación y regresión, clustering, filtrado\n",
    "    colaborativo y recomendación, reducción de dimensionalidad, y\n",
    "    primitivas de bajo nivel\n",
    "\n",
    "-   API de alto nivel para ML pipelines\n",
    "\n",
    "Dos paquetes:\n",
    "\n",
    "-   spark.mllib: API original, basada en RDDs\n",
    "\n",
    "-   spark.ml: API de alto nivel, basada en DataFrames\n",
    "\n",
    "Documentación:\n",
    "[spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)\n",
    "y\n",
    "[spark.apache.org/docs/latest/ml-guide.html](http://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Usa KMeans para agrupar datos de vectores dispersos en dos clusters.\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "import numpy as np\n",
    "\n",
    "# Define un array de 4 vectores dispersos, de 3 elementos cada uno\n",
    "sparse_data = [\n",
    "     SparseVector(3, {1: 1.0}),\n",
    "     SparseVector(3, {1: 1.1}),\n",
    "     SparseVector(3, {2: 1.0}),\n",
    "     SparseVector(3, {2: 1.1})\n",
    " ]\n",
    "\n",
    "# Construye el modelo (agrupa los datos en 2 clusters)\n",
    "model = KMeans.train(sc.parallelize(sparse_data), 2, initializationMode=\"k-means||\",\\\n",
    "                                     seed=50, initializationSteps=5, epsilon=1e-4)\n",
    "\n",
    "print(\"Centros de los clusters: {0}\".format(model.clusterCenters))\n",
    "\n",
    "Test.assertEquals(model.predict(sparse_data[0]),model.predict(sparse_data[1]))\n",
    "Test.assertEquals(model.predict(sparse_data[2]),model.predict(sparse_data[3]))\n",
    "\n",
    "# Salva el modelo en un directorio temporal\n",
    "import os, tempfile\n",
    "path = tempfile.mkdtemp()\n",
    "model.save(sc, path)\n",
    "\n",
    "# Vuelve a cargar el modelo\n",
    "sameModel = KMeansModel.load(sc, path)\n",
    "sameModel.predict(sparse_data[0]) == model.predict(sparse_data[0])\n",
    "Test.assertEquals(sameModel.predict(sparse_data[0]),model.predict(sparse_data[0]))\n",
    "\n",
    "# Borra el directorio temporal\n",
    "from shutil import rmtree\n",
    "try:\n",
    "     rmtree(path)\n",
    "except OSError:\n",
    "     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphX: procesamiento de grafos\n",
    "\n",
    "Programación paralela de grafos con Spark\n",
    "\n",
    "-   Principal abstracción:\n",
    "    [*Graph*](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.graphx.Graph)\n",
    "\n",
    "    -   Multigrafo dirigido con propiedades asignadas a vértices y\n",
    "        aristas\n",
    "\n",
    "    -   Extensión de los RDDs\n",
    "\n",
    "-   Incluye constructores de grafos, operadores básicos (*reverse*,\n",
    "    *subgraph*…) y algoritmos de grafos (*PageRank*, *Triangle\n",
    "    Counting*…)\n",
    "- Actualmente, no disponible en pySpark (solo Scala y Java)\n",
    "\n",
    "Documentación:\n",
    "[spark.apache.org/docs/latest/graphx-programming-guide.html](http://spark.apache.org/docs/latest/graphx-programming-guide.html)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
