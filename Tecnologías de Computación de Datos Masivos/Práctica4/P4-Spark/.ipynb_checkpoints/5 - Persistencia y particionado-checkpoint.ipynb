{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persistencia y particionado\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistencia (*caching*)\n",
    "\n",
    "Problema al usar un RDD varias veces:\n",
    "\n",
    "-   Spark recomputa el RDD y sus dependencias cada vez que se ejecuta\n",
    "    una acción\n",
    "\n",
    "-   Muy costoso (especialmente en problemas iterativos)\n",
    "\n",
    "Solución\n",
    "\n",
    "-   Conservar el RDD en memoria y/o disco\n",
    "\n",
    "-   Métodos `cache()` o `persist()`\n",
    "\n",
    "#### Niveles de persistencia\n",
    "| Nivel                | Espacio  | CPU     | Memoria/Disco   | Descripción\n",
    "| :------------------: | :------: | :-----: | :-------------: | ------------------\n",
    "| MEMORY_ONLY          |   Alto   |   Bajo  |     Memoria     | Guarda el RDD como un objeto Java no serializado en la JVM. Si el RDD no cabe en memoria, algunas particiones no se *cachearán* y serán recomputadas \"al vuelo\" cada vez que se necesiten. Nivel por defecto en Java y Scala.\n",
    "| MEMORY_ONLY_SER      |   Bajo   |   Alto  |     Memoria     | Guarda el RDD como un objeto Java serializado (un *byte array* por partición). Nivel por defecto en Python, usando [`pickle`](http://docs.python.org/2/library/pickle.html).\n",
    "| MEMORY_AND_DISK      |   Alto   |   Medio |     Ambos       | Guarda el RDD como un objeto Java no serializado en la JVM. Si el RDD no cabe en memoria, las particiones que no quepan se guardan en disco y se leen del mismo cada vez que se necesiten\n",
    "| MEMORY_AND_DISK_SER  |   Bajo   |   Alto  |     Ambos       | Similar a MEMORY_AND_DISK pero usando objetos serializados.\n",
    "| DISK_ONLY            |   Bajo   |   Alto  |     Disco       | Guarda las particiones del RDD solo en disco.\n",
    "| OFF_HEAP             |          |         |                 | Guarda el RDD serializado usando [`Tachyon`](http://tachyon-project.org/).\n",
    "   \n",
    "Definidos en\n",
    "[`pyspark.StorageLevel`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel)\n",
    "    \n",
    "#### Nivel de persistencia\n",
    "\n",
    "-   En Scala y Java, el nivel por defecto es MEMORY\\_ONLY\n",
    "\n",
    "-   En Python, el nivel por defecto es MEMORY\\_ONLY\\_SER\n",
    "\n",
    "    -   Por defecto, Python serializa los datos como objetos *pickled*\n",
    "\n",
    "- Es posible especificar otra serialización al crear el SparkContext\n",
    "```python\n",
    "sc = SparkContext(master=\"local\", appName=\"Mi app\", serializer=pyspark.MarshalSerializer())\n",
    "```\n",
    "    \n",
    "#### Recuperación de fallos\n",
    "\n",
    "-   Si falla un nodo con datos almacenados, el RDD se recomputa\n",
    "\n",
    "    -   Añadiendo `_2` al nivel de persistencia, se guardan 2 copias del\n",
    "        RDD\n",
    "        \n",
    "#### Gestión de la cache\n",
    "\n",
    "-   Algoritmo LRU para gestionar la cache\n",
    "\n",
    "    -   Para niveles *solo memoria*, los RDDs viejos se eliminan y\n",
    "        recalculan\n",
    "\n",
    "    -   Para niveles *memoria y disco*, las particiones que no caben se\n",
    "        escriben a disco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "1 test passed.\n",
      "Nivel de persistencia de rdd: Disk Memory Serialized 2x Replicated \n",
      "Nivel de persistencia de rdd2: Memory Serialized 1x Replicated\n",
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "from test_helper import Test\n",
    "from __future__ import print_function\n",
    "\n",
    "rdd = sc.parallelize(range(1000), 10)\n",
    "Test.assertTrue(not rdd.is_cached)\n",
    "\n",
    "rdd.persist(StorageLevel.MEMORY_AND_DISK_SER_2)\n",
    "Test.assertTrue(rdd.is_cached)\n",
    "\n",
    "rdd2 = rdd.map(lambda x: x*x)\n",
    "Test.assertTrue(not rdd2.is_cached)\n",
    "\n",
    "rdd2.cache() # Nivel por defecto\n",
    "Test.assertTrue(rdd2.is_cached)     \n",
    "\n",
    "print(\"Nivel de persistencia de rdd: {0} \".format(rdd.getStorageLevel()))\n",
    "print(\"Nivel de persistencia de rdd2: {0}\".format(rdd2.getStorageLevel()))\n",
    "\n",
    "rdd.unpersist() # Sacamos rdd de la cache\n",
    "Test.assertTrue(not rdd.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Particionado\n",
    "\n",
    "El número de particiones es función del tamaño del cluster o el número de\n",
    "bloques del fichero en HDFS\n",
    "\n",
    "-   Es posible ajustarlo al crear u operar sobre un RDD\n",
    "\n",
    "-   El paralelismo de RDDs que derivan de otros depende del de sus RDDs\n",
    "    padre\n",
    "\n",
    "-   Dos funciones útiles:\n",
    "\n",
    "    -   `rdd.getNumPartitions()` devuelve el número de particiones del\n",
    "        RDD\n",
    "\n",
    "    -   `rdd.glom()` devuelve un nuevo RDD juntando los elementos de\n",
    "        cada partición en una lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n",
      "1 test passed.\n",
      "Pairs con 4 particiones: [[(1, 1)], [(2, 2), (3, 3)], [(4, 4), (2, 2)], [(4, 4), (1, 1)]]\n",
      "Reducción con 4 particiones: [[(4, 8)], [(1, 2)], [(2, 4)], [(3, 3)]]\n",
      "Reducción con 2 particiones: [[(2, 4), (4, 8)], [(1, 2), (3, 3)]]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 2, 4, 1], 4)\n",
    "pairs = rdd.map(lambda x: (x, x))\n",
    "Test.assertEquals(pairs.collect(), [(1,1),(2,2),(3,3),(4,4),(2,2),(4,4),(1,1)])\n",
    "Test.assertEquals(pairs.getNumPartitions(), 4)\n",
    "\n",
    "print(\"Pairs con 4 particiones: {0}\".format(\n",
    "        pairs.glom().collect()))\n",
    "\n",
    "# Reducción manteniendo el número de particiones\n",
    "print(\"Reducción con 4 particiones: {0}\".format(\n",
    "        pairs.reduceByKey(lambda x, y: x+y).glom().collect()))\n",
    "# Reducción modificando el número de particiones\n",
    "\n",
    "print(\"Reducción con 2 particiones: {0}\".format(\n",
    "        pairs.reduceByKey(lambda x, y: x+y, 2).glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funciones de reparticionado\n",
    "- `repartition(n)` devuelve un nuevo RDD que tiene exactamente `n` particiones\n",
    "- `coalesce(n)` más eficiente que `repartition`, minimiza el movimiento de datos\n",
    "    - Solo permite reducir el número de particiones\n",
    "- `partitionBy(n,[partitionFunc])` Particiona por clave, usando una función de particionado (por defecto, un hash de la clave)\n",
    "    - Para RDDs clave/valor\n",
    "    - La misma clave a la misma partición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs con 5 particiones: [[(2, 2), (4, 4)], [(1, 1), (2, 2), (1, 1)], [(3, 3)], [], [(4, 4)]]\n",
      "Pairs con 2 particiones: [[(1, 1), (2, 2), (3, 3)], [(4, 4), (2, 2), (4, 4), (1, 1)]]\n",
      "Particionado por clave (2 particiones): [[(2, 2), (4, 4), (2, 2), (4, 4)], [(1, 1), (3, 3), (1, 1)]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pairs con 5 particiones: {0}\".format(\n",
    "        pairs.repartition(5).glom().collect()))\n",
    "\n",
    "print(\"Pairs con 2 particiones: {0}\".format(\n",
    "        pairs.coalesce(2).glom().collect()))\n",
    "\n",
    "print(\"Particionado por clave (2 particiones): {0}\".format(\n",
    "        pairs.partitionBy(2).glom().collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
