{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura y escritura de ficheros\n",
    "==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sistemas de ficheros soportados\n",
    "-   Igual que Hadoop, Spark soporta diferentes filesystems: Local, HDFS,\n",
    "    Amazon S3\n",
    "\n",
    "    -   En general, soporta cualquier fuente de datos que ofrezca un\n",
    "        Hadoop InputFormat\n",
    "\n",
    "-   También, acceso a bases de datos estructuradas o no estructuradas\n",
    "\n",
    "    -   MySQL, Postgres, etc. mediante JDBC\n",
    "\n",
    "    -   Apache Hive, HBase, Cassandra o Elasticsearch\n",
    "\n",
    "### Formatos de fichero soportados\n",
    "\n",
    "-   Spark puede acceder a diferentes tipos de ficheros:\n",
    "\n",
    "    -   Texto plano, JSON, CSV, ficheros sequence, *protocol buffers* y\n",
    "        *object files*\n",
    "\n",
    "    -   Igual que Hadoop, soporta ficheros comprimidos (mismas\n",
    "        consideraciones sobre formatos “splittables”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ficheros de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El fichero pg9980.txt.bz2 tiene  34014 palabras\n",
      "El fichero pg16625.txt.bz2 tiene 170900 palabras\n",
      "El fichero pg5201.txt.bz2 tiene  49441 palabras\n",
      "El fichero pg25807.txt.bz2 tiene  15014 palabras\n",
      "El fichero pg32315.txt.bz2 tiene  46142 palabras\n",
      "El fichero pg24536.txt.bz2 tiene 134016 palabras\n",
      "El fichero pg2000.txt.bz2 tiene 384258 palabras\n",
      "El fichero pg1619.txt.bz2 tiene 109878 palabras\n",
      "El fichero pg7109.txt.bz2 tiene  35037 palabras\n",
      "El fichero pg18005.txt.bz2 tiene  86446 palabras\n",
      "El fichero pg17073.txt.bz2 tiene 309473 palabras\n",
      "El fichero pg14329.txt.bz2 tiene 183777 palabras\n",
      "El fichero pg25640.txt.bz2 tiene 207338 palabras\n",
      "El fichero pg8870.txt.bz2 tiene  54348 palabras\n",
      "El fichero pg17013.txt.bz2 tiene 396086 palabras\n"
     ]
    }
   ],
   "source": [
    "# Lee todos los ficheros del directorio\n",
    "# y crea una lista particionada de líneas\n",
    "lineas = sc.textFile(\"datos/libros-mini/*\")\n",
    "words = lineas.flatMap(lambda x: x.split(\" \"))\n",
    "# Salva el RDD words en varios ficheros de salida\n",
    "# (un fichero por partición)\n",
    "words.saveAsTextFile(\"file:///tmp/txtoutdir4\")\n",
    "        \n",
    "# Lee ficheros y devuelve un par clave/valor\n",
    "# clave->nombre fichero, valor->fichero completo\n",
    "rdd=sc.wholeTextFiles(\"datos/libros-mini/*\")\n",
    "# Obtiene un lista clave/valor\n",
    "# clave->nombre fichero, valor->numero de palabras\n",
    "lista = rdd.mapValues(lambda x: len(x.split())).collect()\n",
    "for libro in lista:\n",
    "    print(\"El fichero {0:14s} tiene {1:6d} palabras\"\n",
    "          .format(libro[0].split(\"/\")[-1], libro[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ficheros JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = sc.textFile(\"datos/info.json\")\n",
    "import json\n",
    "data = input.map(lambda x: json.loads(x))\n",
    "(data.filter(lambda x: \"client\" in x and x[\"client\"])\n",
    "            .map(lambda x: json.dumps(x))\n",
    "            .saveAsTextFile(\"file:///tmp/jsonoutdir\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ficheros Sequence\n",
    "Ficheros clave/valor usados en Hadoop\n",
    "\n",
    "-   Sus elementos implementan la interfaz `Writable`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'a', 2), (u'a', 8), (u'b', 5)]\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize([(\"a\",2), (\"b\",5), (\"a\",8)])\n",
    "# Salvamos el RDD como fichero secuence\n",
    "data.saveAsSequenceFile(\"file:///tmp/sequenceoutdir\")\n",
    "# Lo leemos en otro RDD\n",
    "rdd = sc.sequenceFile(\"file:///tmp/sequenceoutdir\", \n",
    "                      \"org.apache.hadoop.io.Text\", \n",
    "                      \"org.apache.hadoop.io.IntWritable\")\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatos de entrada/salida de Hadoop\n",
    "Spark puede interactuar con cualquier formato de fichero soportado por Hadoop \n",
    "- Soporta las APIs “vieja” y “nueva”\n",
    "- Permite acceder a otros tipos de almacenamiento (no fichero), p.e. HBase o MongoDB, a través de `saveAsHadoopDataSet` y/o `saveAsNewAPIHadoopDataSet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'b', u'5'), (u'a', u'2'), (u'a', u'8')]\n"
     ]
    }
   ],
   "source": [
    "# Salvamos el RDD como fichero de texto Hadoop (TextOutputFormat)\n",
    "data.saveAsNewAPIHadoopFile(\"file:///tmp/hadoopfileoutdir\", \n",
    "                            \"org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\",\n",
    "                            \"org.apache.hadoop.io.Text\",\n",
    "                            \"org.apache.hadoop.io.IntWritable\")\n",
    "# Lo leemos como fichero clave-valor Hadoop (KeyValueTextInputFormat)\n",
    "rdd = sc.newAPIHadoopFile(\"file:///tmp/hadoopfileoutdir\", \n",
    "                          \"org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\",\n",
    "                          \"org.apache.hadoop.io.Text\",\n",
    "                          \"org.apache.hadoop.io.IntWritable\")\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object files\n",
    "\n",
    "Ficheros binarios que guardan cualquier tipo de RDDs (no solo\n",
    "clave/valor)\n",
    "\n",
    "-   Usan serialización Java (en Java y Scala, métodos\n",
    "    `saveAsObjectFile()` y `objectFile()`)\n",
    "\n",
    "-   En Python, se usa *pickle* (métodos `saveAsPickleFile()` y\n",
    "    `pickleFile()`)\n",
    "\n",
    "-   Solo útiles para comunicar trabajos Spark entre sí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Práctica 4\n",
    "\n",
    "A partir del fichero apat63_99.txt, crear un fichero secuencia apat63_99.seq con clave=país (campo 4) valor=n_patente,año (campos 0 y 1), ambos de tipo string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test passed.\n"
     ]
    }
   ],
   "source": [
    "from test_helper import Test\n",
    "\n",
    "#Borramos el directorio de salida si existe (comando Linux)\n",
    "!rm -rf /tmp/apat63_99.seq\n",
    "\n",
    "data = sc.textFile(\"datos/patentes-mini/apat63_99.txt\")\n",
    "\n",
    "prdd = data.map(lambda x: (x.split(\",\")[4].encode('utf-8'),x.split(\",\")[0].encode('utf-8')+\\\n",
    "                           \",\"+x.split(\",\")[1].encode('utf-8')))\n",
    "\n",
    "pairs = prdd.filter(lambda x: x[0] != '\\\"COUNTRY\\\"')\n",
    "\n",
    "#print (pairs.collect())\n",
    "\n",
    "pairs.saveAsSequenceFile(\"file:///tmp/apat63_99.seq\")\n",
    "\n",
    "# Lo leemos para comprobar que se guardó bien en otro RDD\n",
    "rdd = sc.sequenceFile(\"file:///tmp/apat63_99.seq\", \n",
    "                      \"org.apache.hadoop.io.Text\", \n",
    "                      \"org.apache.hadoop.io.Text\")\n",
    "\n",
    "#print(rdd.collect())\n",
    "\n",
    "#Comprobamos que lo guardado en el sequence y lo leído son iguales\n",
    "Test.assertEquals(pairs.collect(), rdd.collect())\n",
    "\n",
    "#NOTA: Python detecta como iguales las dos colecciones porque todos los elementos son iguales\n",
    "#aunque el primer RDD se codifique en UTF-8 y el segundo RDD se lea como Unicode. Para Python\n",
    "#esto es indiferente y cuando se recorren los elementos se pueden decodificar o codificar a \n",
    "#UTF-8 o a cualquier otro formato si se desea."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
