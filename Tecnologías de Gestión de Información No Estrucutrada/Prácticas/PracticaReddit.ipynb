{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primera práctica de TGINE: TF-IDF con una subred de Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La Subred elegida debido a su gran potencial de texto para generar un buen corpus ha sido la de 'lotr' (Lord of the Rings) ya que contaba con muchos posts y comentarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La primera parte de la práctica consistía en leer de dicha subred los post y comentarios que después serían los documentos que le pasaríamos posteriormente al vectorizador, escogiendo para ellos los más recientes y los más populares. Pero para no estar llamando a la API de Reddit en cada ejecución se opta por hacer un primer script de Python que guarde la información obtenida en un fichero de datos semiestructurados como es XML en disco, y después en un segundo script leer del mismo sin necesidad de llamar a la API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para ello se define el siguiente XML Schema, que luego nos servirá para validar que el documento está bien estructurado antes de guardarlo en disco.\n",
    "\n",
    "- esquema-reddit.xsd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<xs:schema xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" targetNamespace=\"practicaReddit\" xmlns=\"practicaReddit\" elementFormDefault=\"qualified\">\n",
    "  <xs:element name=\"subreddit\">\n",
    "    <xs:complexType>\n",
    "      <xs:sequence>\n",
    "        <xs:element name=\"apartado\" minOccurs=\"2\" maxOccurs=\"2\">\n",
    "          <xs:complexType>\n",
    "            <xs:sequence>\n",
    "              <xs:element name=\"post\" minOccurs=\"1\" maxOccurs=\"unbounded\">\n",
    "                <xs:complexType>\n",
    "                  <xs:sequence>\n",
    "                    <xs:element name=\"comentario\" minOccurs=\"0\" maxOccurs=\"unbounded\">\n",
    "                    \t<xs:complexType>\n",
    "              \t\t\t\t<xs:attribute name=\"autor\" type=\"xs:string\" />\n",
    "              \t\t\t\t<xs:attribute name=\"fecha\" type=\"xs:dateTime\" />\n",
    "              \t\t\t\t<xs:attribute name=\"texto\" type=\"xs:string\" />\n",
    "              \t\t\t</xs:complexType>\t\n",
    "                    </xs:element>\n",
    "                  </xs:sequence>\n",
    "                  <xs:attribute name=\"titulo\" type=\"xs:string\" />\n",
    "              \t  <xs:attribute name=\"autor\" type=\"xs:string\" />\n",
    "              \t  <xs:attribute name=\"fecha\" type=\"xs:dateTime\" />\n",
    "              \t  <xs:attribute name=\"texto\" type=\"xs:string\" />\n",
    "                </xs:complexType>\n",
    "              </xs:element>\n",
    "            </xs:sequence>\n",
    "            <xs:attribute name=\"nombre\" type=\"xs:string\" />\n",
    "          </xs:complexType>\n",
    "        </xs:element>\n",
    "      </xs:sequence>\n",
    "      <xs:attribute name=\"nombre\" type=\"xs:string\" />\n",
    "    </xs:complexType>\n",
    "  </xs:element>\n",
    "</xs:schema>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Como se puede apreciar el XML esquema sigue un orden jeráquico de los elementos que nos vamos a encontrar en la subred. El elemento 'subreddit' sería el elemento raíz que recoge un atributo nombre para la red, en este caso será 'lotr'. Después tendrá dos hijos, denominados 'apartados' que recogerán los post más recientes por una parte y los más populares por otra. En cada uno de los apartados estarán los posts correspondientes, y dentro de los posts sus comentarios. A efectos de la lógica queda de este modo bien definida la jerarquía de la subred. No obstante, para el vectorizador tanto posts como comentarios serán documentos individuales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer Script: practicaRedditCreacionXML.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Nov 28 22:48:20 2016\n",
    "\n",
    "@author: moises\n",
    "\"\"\"\n",
    "#Importación de librerías\n",
    "import sys\n",
    "import praw\n",
    "import datetime\n",
    "import xml.etree.ElementTree as ET\n",
    "from lxml import etree\n",
    "from pprint import pprint\n",
    "\n",
    "#Definición y cuerpo de las funciones\n",
    "def conectarAReddit():\n",
    "    UA = 'linux:practicaReddit.py:v1.0 (by /u/Zollkron)'\n",
    "    wrapper = praw.Reddit(UA)\n",
    "    wrapper.set_oauth_app_info(client_id='Nmmtds24J7nFhw',\n",
    "                      client_secret='ebYaWc_8lSs1mghubwbGvCWDYvw',\n",
    "                      redirect_uri='http://127.0.0.1:65010/'\n",
    "                                   'authorize_callback')\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tal y como se pide en el enunciado se crea una primera función para conectar a Reddit usando la librería praw y usando la autenticación oauth en lugar de la ya desfasada autenticación mediante usuario y contraseña."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTA: Se usa pprint para la identificación de los atributos de los objetos devueltos por reddit ya que en la documentación  no he podido encontrar nada que me lo indicase. Se deja como comentario para que no interfiera en el la ejecución del script, pero me gustaría dejarlo constar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def obtenerSubReddit(wrapper, nombre):\n",
    "    return wrapper.get_subreddit(nombre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tras esto se crea una función para desde el wrapper de praw obtener la subred indicada por parámetro, en nuestro caso 'lotr'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def imprimirSubmissions(submissions):\n",
    "    for submission in submissions:\n",
    "        print(\"-==Post==-\")\n",
    "        print(\"\")\n",
    "        creation_date = datetime.datetime.fromtimestamp(int(submission.created))\n",
    "        print(creation_date)\n",
    "        print(\"\")\n",
    "        print(submission.title)\n",
    "        print(\"\")\n",
    "        print(submission.selftext)\n",
    "        print(\"\")\n",
    "        print(submission.author)\n",
    "        print(\"\")\n",
    "        #pprint(vars(submission))\n",
    "        for comment in submission.comments:\n",
    "            print(\"-==Comment==-\")\n",
    "            print(\"\")\n",
    "            creation_date = datetime.datetime.fromtimestamp(int(comment.created))\n",
    "            print(creation_date)\n",
    "            print(\"\")\n",
    "            print(comment.body)\n",
    "            print(\"\")\n",
    "            print(comment.author)\n",
    "            print(\"\")\n",
    "            #pprint(vars(comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### He creado una función auxiliar denominada imprimirSubmissions que muestra por consola los posts así como sus comentarios para comprobar en primera instancia que el wrapper ha funcionado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def construirXML(subreddit):\n",
    "    xmlnsUri = \"practicaReddit\"\n",
    "    xmlnsXsi = \"http://www.w3.org/2001/XMLSchema-instance\"\n",
    "    xsiSchema = \"practicaReddit practicaReddit.xsd\"\n",
    "    raiz = ET.Element(\"subreddit\", nombre=subreddit.display_name, xmlns=xmlnsUri, \n",
    "                  **{'xmlns:xsi':xmlnsXsi,'xsi:schemaLocation':xsiSchema})\n",
    "    #Se crea un elemento apartado para los posts más recientes dentro del XML\n",
    "    postRecientes = ET.SubElement(raiz, \"apartado\", nombre=\"Posts Mas Recientes\")\n",
    "    #Se obtienen los posts junto con sus comentarios a través del objeto subreddit obtenido mediante el wrapper\n",
    "    submissions = subreddit.get_hot() #Ultimos posts\n",
    "    #Para cada Post\n",
    "    for submission in submissions:\n",
    "        #Obtenemos su fecha de creación\n",
    "        creation_date = datetime.datetime.fromtimestamp(int(submission.created))\n",
    "        #La formateamos de modo que nos la acepte el tipo datetime de XML\n",
    "        fecha_formateada = creation_date.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        #Y creamos un elemento post dentro del documento XML en memoria\n",
    "        post = ET.SubElement(postRecientes, \"post\", titulo=submission.title , \\\n",
    "                             autor=submission.author.name,fecha=fecha_formateada,texto=submission.selftext)\n",
    "        #Para cada comentario dentro del Post\n",
    "        for comment in submission.comments:\n",
    "            #Obtenemos su fecha de creación\n",
    "            creation_date = datetime.datetime.fromtimestamp(int(comment.created))\n",
    "            #La formateamos de modo que nos la acepte el tipo datetime de XML\n",
    "            fecha_formateada = creation_date.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "            #Y creamos un elemento comentario dentro del documento XML en memoria\n",
    "            ET.SubElement(post, \"comentario\", autor=comment.author.name, \\\n",
    "                          fecha=fecha_formateada, texto=comment.body)\n",
    "    #Se repite el mismo proceso para los posts más populares\n",
    "    postPopulares = ET.SubElement(raiz, \"apartado\", nombre=\"Posts Mas Populares\")\n",
    "    submissions = subreddit.get_top() #Post Mas Populares\n",
    "    for submission in submissions:\n",
    "        creation_date = datetime.datetime.fromtimestamp(int(submission.created))\n",
    "        fecha_formateada = creation_date.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        post = ET.SubElement(postPopulares, \"post\", titulo=submission.title, \\\n",
    "                      autor=submission.author.name, fecha=fecha_formateada, \\\n",
    "                      texto=submission.selftext)\n",
    "        for comment in submission.comments:\n",
    "            creation_date = datetime.datetime.fromtimestamp(int(comment.created))\n",
    "            fecha_formateada = creation_date.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "            ET.SubElement(post, \"comentario\", autor=comment.author.name, \\\n",
    "                          fecha=fecha_formateada, texto=comment.body)\n",
    "    #Por último devolvemos el elemento raíz del documento XML\n",
    "    return raiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La siguiente función, construirXML, es el corazón de este primer script, y es el que se encarga de construir el XML de toda la subred obtenido primero en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validarXML(xml):\n",
    "   fichero = open(\"esquema-reddit.xsd\",\"r\")\n",
    "   schema = etree.XML(fichero.read())\n",
    "   xmlSchema = etree.XMLSchema(schema)\n",
    "   xmlParser = etree.XMLParser(schema = xmlSchema)\n",
    "   try:\n",
    "       etree.fromstring(ET.tostring(xml), xmlParser) \n",
    "   except etree.XMLSyntaxError:\n",
    "       mensaje = \"El XML no se ha validado porque:\\n\" + str(sys.exc_info()[1])\n",
    "       print(mensaje)\n",
    "       return False     \n",
    "   else:\n",
    "       mensaje = \"El XML generado es correcto.\"\n",
    "       print(mensaje)\n",
    "       return True    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tras ello hay una función de validarXML para asegurarnos que el XML construido en memoria es correcto según el esquema que hemos definido antes de persistirlo en disco. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Principio de la identación de Python en este script (que coloquialmente podemos asimilar a la función main)\n",
    "wrapper = conectarAReddit()\n",
    "subreddit = obtenerSubReddit(wrapper, 'lotr')\n",
    "#pprint(vars(subreddit))\n",
    "print(subreddit.display_name)\n",
    "#submissions = subreddit.get_hot() #Ultimos posts\n",
    "#imprimirSubmissions(submissions)\n",
    "\n",
    "xml = construirXML(subreddit)\n",
    "resultado = validarXML(xml)\n",
    "if resultado == True:\n",
    "    print(\"Guardamos\")\n",
    "    from xml.dom import minidom\n",
    "    xmlstr = minidom.parseString(ET.tostring(xml)).toprettyxml(indent=\"   \")\n",
    "    with open(\"LordOfTheRings.xml\", \"w\") as f:\n",
    "        f.write(xmlstr.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lo que viene a continuación se podría decir que es \"la función main\" del script, pero para Python en realidad es el principio de su identación en este script. Tras generar el XML ya tenemos los datos necesarios para leerlo, extraer su corpus y pasaro por el vectorizador de scikit-learn, y eso es justo lo que hará el segundo script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo Script: practicaRedditVectorizacion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Dec  6 18:05:54 2016\n",
    "\n",
    "@author: moises\n",
    "\"\"\"\n",
    "#Importación de librerías\n",
    "import operator\n",
    "from lxml import etree\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Definición y cuerpo de las funciones\n",
    "def leerXML(ruta):\n",
    "    return etree.parse(ruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La primera función sólo se limita a leer el contenido del fichero XML guardado en disco por el anterior script, en nuestro caso particular es 'LordOfTheRings.xml'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extraerCorpus(raiz):\n",
    "    corpus = []\n",
    "    for apartado in raiz:\n",
    "        for post in apartado:\n",
    "            #print post.get('texto')\n",
    "            corpus.append(post.get('texto'))\n",
    "            for comentario in post:\n",
    "                #print comentario.get('texto')\n",
    "                corpus.append(comentario.get('texto'))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La segunda función extraerCorpus es para que construya una colección de documentos de texto tanto de los posts como de los comentarios, cada uno formando un documento detexto individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Principio de la identación de Python en este script (que coloquialmente podemos asimilar a la función main)\n",
    "\n",
    "#Leemos el documento XML creado con el script anterior\n",
    "documentoXML = leerXML(\"LordOfTheRings.xml\")\n",
    "#Extraemos el elemento raíz\n",
    "raiz = documentoXML.getroot()\n",
    "#A partir de dicho elemento raíz extraemos el Corpus del texto\n",
    "corpus = extraerCorpus(raiz)\n",
    "#print corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos el fichero XML guardado en disco por el anterior script, en nuestro caso particular es 'LordOfTheRings.xml'. Tras ello, obtenemos el elemento raíz correspondiente con la subred 'lotr'. A continuación, le pasamos el elemento raíz a la función extraerCorpus para que construya una colección de documentos de texto. Esta colección de documentos de texto, a la que llamamos Corpus, es lo que le pasaremos al vectorizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creamos el vectorizador para que analice por palabra, y que filtre todas aquellas\n",
    "#palabras que aparezcan en menos de 10 documentos así como las stop_words en inglés\n",
    "vectorizer = TfidfVectorizer(analyzer='word', min_df = 10, stop_words = 'english')\n",
    "#Usando el vectorizador extraemos la matriz vectorizada de todas las palabras\n",
    "matrizVectorizada = vectorizer.fit_transform(corpus)\n",
    "#Usamos también el vectorizador para extraer todas las palabras que haya encontrado\n",
    "palabras = vectorizer.get_feature_names()\n",
    "#print palabras\n",
    "#print matrizVectorizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A continuación, nos creamos un vectorizador del tipo TfidfVectorizer indicándole que lo que queremos es analizar palabras, y que filtre todas aquellas que aparezcan en menos de 10 documentos y las stop-words en inglés. Una vez preparado el vectorizador, lo usamos para obtener la matriz vectorizada a partir de nuestro corpus, de paso obtenemos un listado de todas las palabras que ha encontrado que nos será útil después."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Como la matriz vectorizada que nos devuelve el vectorizador está comprimida\n",
    "#para ahorrar espacio, obtenemos la matriz de densidad para poder acceder mejor \n",
    "#a los datos\n",
    "matrizDensa = matrizVectorizada.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La matriz vectorizada que nos devuelve el vectorizador está comprimida para ahorrar espacio, por ello si queremos tratar los datos que contiene es mejor obtener su matriz densa. En dicha matriz ya tenemos todo lo que necesitamos: las puntuaciones TF-IDF de las palabras de todo el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Construimos un mapa para guardar todas las palabras del corpus\n",
    "#así como su puntuación TF-IDF. De momento la inicializamos a 0 de puntuación.\n",
    "mapaPalabras = {palabra.encode(\"utf-8\"):float(0) for palabra in palabras}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nos creamos a partir del listado de palabras encontradas en el corpus por el vectorizador un mapa(clave,valor) en el que la clave será la palabra en cuestión (codificada en 'utf-8'), y el valor la suma acumulada TF-IDF que nos encontremos al recorrer la matriz densa. Al principio inicializamos todos los valores del mapa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idDocumento = 0\n",
    "#Empezamos a recorrer los documentos\n",
    "for doc in matrizDensa:\n",
    "    #print \"Documento %d\" %(idDocumento)\n",
    "    idPalabra = 0\n",
    "    lista = doc.tolist()\n",
    "    #Nos aseguramos de recorrer todas las filas de la matriz del documento\n",
    "    for i in range(len(lista)):\n",
    "        #Recorremos las palabras de la fila actual (i)\n",
    "        for puntuacionTfIdf in doc.tolist()[i]:\n",
    "            #Para cada palabra comprobamos si su tf-idf es mayor que 0\n",
    "            if puntuacionTfIdf > 0:\n",
    "                palabra = palabras[idPalabra]\n",
    "                #Sumamos la puntuación TF-IDF de este documento a la palabra\n",
    "                mapaPalabras[palabra.encode(\"utf-8\")] += float(puntuacionTfIdf)\n",
    "            idPalabra +=1\n",
    "    idDocumento +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tras ello recorremos la matriz densa, primero documento a documento, después fila a fila de cada submatriz del documento, y después ya sí por fin, palabra por palabra. Si vemos que la palabra que estamos observando tiene un valor de TF-IDF superior a 0, lo acumulamos en su acumulador correspondiente dentro del mapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Llegado a este punto ya tenemos todas las palabras con su puntuación TF-IDF total\n",
    "#respecto a TODO el corpus. Tras esto tenemos que ordenarlas de mayor a menor\n",
    "#puntuación IF-IDF y quedarnos con las 10 primeras\n",
    "palabrasCentrales = sorted(mapaPalabras.items(), key=operator.itemgetter(1), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una vez terminada de recorrer toda la matriz densa ya tenemos todas las puntuaciones TF-IDF acumuladas por palabras en el mapa, pero el mapa está desordenado y sólo nos interesa las primeras 10 palabras centrales, que son las que mayor suma acumulada de TF-IDF tienen. Y justo eso es lo que hacemos al final, ordenamos el mapa por valor de forma decreciente (de mayor a menor puntuación TF-IDF) y le decimos que nos quedamos con los 10 primeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tras quedarnos con las palabras centrales las mostramos así como su puntuación\n",
    "print('{0: <10} {1}'.format(\"Palabra\", \"TF-IDF\"))\n",
    "print \"-------  ----------------\"\n",
    "for palabra, puntuacionTfIdf in palabrasCentrales:\n",
    "    print('{0: <10} {1}'.format(palabra, puntuacionTfIdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tras ello mostramos finalmente por consola y de forma formateada las 10 palabras centrales. Siendo este el resultado final en mi caso en el momento de probarlo:\n",
    "\n",
    "- Palabra    TF-IDF\n",
    "- -------  ----------------\n",
    "- like       13.1177156153\n",
    "- just       12.6630773332\n",
    "- great      11.8270984841\n",
    "- com        11.0187180747\n",
    "- movie      10.2632426618\n",
    "- did        9.48731187443\n",
    "- love       8.25114118716\n",
    "- work       8.11053026027\n",
    "- sauron     7.76398522288\n",
    "- fellowship 7.50957198986"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A la vista del resultado obtenido podemos observar como dentro de los términos más usados en el corpus según su puntuación TF-IDF se expresan sentimientos como 'like', 'great' o 'love' lo que indica un cierto amor de los fans por el mundo de El Señor de los Anillos. También se nombra la película 'movie' o el trabajo 'work' (obra) refiriéndose a los libros. Se nombra al antagonista 'sauron' quien es en realidad El Señor de los Anillos que los controla todos con el anillo único. Y por último sale la palabra 'fellowship' que es la compañía que se forma para destruirlo. También se 'cuelan' términos que a priori no tienen nada que ver con el mundo de ESDLA, como 'just', 'com', o 'did' que pueden haber salido por resultar expresiones muy comunes en inglés, y en el caso de 'com' podría ser que se refiriese a un apócope de 'company' (compañía) que ya no queda claro si es para referirse a la compañía del anillo, a la que el autor del libro denomina 'fellowship', o si se trata de la compañía que hace la película, o algún otro tipo de compañía que se nombra en esa subred. La cuestión es que la técnica TF-IDF ha sido bastante acertada con la mayoría de los términos que identifican al mundo de el ESDLA, ya que son realmente significativos por lo menos en lo que a los datos extraidos de esa subred se refiere."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
